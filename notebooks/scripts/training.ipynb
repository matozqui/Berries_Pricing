{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599838866442",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA AND ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(dataset, p_values, d_values, q_values, crop, ctry, dfNullID):\n",
    "\n",
    "    ##  Function to evaluate several combinations of p, d and q values in ARIMA model and select the order with the least MAE ##\n",
    "\n",
    "\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\tfrom sklearn.metrics import mean_squared_error\n",
    "\tfrom sklearn.metrics import mean_absolute_error\n",
    "\timport warnings\n",
    "\timport pandas as pd\n",
    "\timport pyodbc\n",
    "\tfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\tfrom pandas import read_csv\n",
    "\tfrom pandas import datetime\n",
    "\tfrom statsmodels.tsa.arima_model import ARIMA\n",
    "\timport numpy as np\n",
    "\timport pmdarima as pm\n",
    "\tfrom pmdarima import model_selection\n",
    "\timport datetime\n",
    "\tfrom datetime import datetime, timedelta\n",
    "\timport matplotlib.pyplot as plt\n",
    "\n",
    "\tdataset = dataset.astype('float32')\n",
    "\tbest_score_mae, score_rmse, score_bias, best_cfg = float(\"inf\"), float(\"inf\"), float(\"inf\"), None\n",
    "\n",
    "\tfor p in p_values:\n",
    "\t\tfor d in d_values:\n",
    "\t\t\tfor q in q_values:\n",
    "\t\t\t\torder = (p,d,q)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmae,rmse,bias = evaluate_arima_model(dataset, order, dfNullID)\n",
    "\t\t\t\t\tif mae < best_score_mae: #mae is the chosen measure for selecting best order\n",
    "\t\t\t\t\t\tbest_score_mae, score_rmse, score_bias, best_cfg = mae, rmse, bias, order\n",
    "\t\t\t\t\tprint('ARIMA%s MAE=%.3f RMSE=%.3f BIAS=%.3f' % (order,mae,rmse,bias))\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\tprint('Best ARIMA%s MAE=%.3f RMSE=%.3f BIAS=%.3f' % (best_cfg, best_score_mae, score_rmse, score_bias))\n",
    "\tversions_file = '../../data/03_models/Model_versions.txt'\n",
    "\tmodel_data = 'Best ARIMA%s MAE=%.3f RMSE=%.3f BIAS=%.3f' % (best_cfg, best_score_mae, score_rmse, score_bias)\n",
    "\tupdated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\twith open(versions_file, \"a\") as f:\n",
    "\t\tf.write(\"##\"+crop+\" \"+ctry+\" // \"+model_data+\"Updated \"+updated+\"##\\n\")\n",
    "\t\n",
    "\treturn(best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_arima_model(X, arima_order, dfNullID):\n",
    "\n",
    "    ##  Function to evaluate a ARIMA, SARIMA and SARIMAX models ##\n",
    "\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    import warnings\n",
    "    # https://stackoverflow.com/questions/34444607/how-to-ignore-statsmodels-maximum-likelihood-convergence-warning \n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    warnings.simplefilter('ignore', ConvergenceWarning) \n",
    "\n",
    "    # prepare training dataset\n",
    "    X_clean = pd.DataFrame(X)[~pd.DataFrame(X).index.isin(dfNullID['ID'])].values # Pick only campaign weeks for measure the prediction error\n",
    "\n",
    "    train_size = int(len(X_clean) * 0.66)\n",
    "    train, test = X_clean[0:train_size], X_clean[train_size:]\n",
    "    history = [x for x in train]\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model_fit = SARIMAX(history, order=arima_order).fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    mae = mean_absolute_error(test, predictions) #MAE: average of the forecast error values in absolute values\n",
    "    rmse = np.sqrt(mean_squared_error(test, predictions)) # Root mean square error: average of the squared forecast error values\n",
    "    bias = np.mean(predictions-test) # Bias measure, suggesting tendency of the model to over forecast (positive error) or under forecast (negative error)\n",
    "    return mae,rmse,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_arima_model(crop,ctry,trade_ctry,ctgr,mdel):\n",
    "\n",
    "    ##  Function to train a ARIMA model and save it as a pickle .pkl file ## \n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import transformations as transf\n",
    "    import extractions as extract\n",
    "    import config\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    crop_lc = crop.lower()\n",
    "    ctry_lc = ctry.lower()\n",
    "    tctr_lc = trade_ctry.lower()\n",
    "    ctgr_lc = ctgr.lower()\n",
    "    mdel_lc = mdel.lower()\n",
    "\n",
    "    # Get prices interpolated\n",
    "    df_prices = extract.get_prices_interpolated(crop,ctry,trade_ctry,ctgr)\n",
    "\n",
    "    # Save null indexes with their original index id\n",
    "    dfNullID = extract.get_null_prices(crop,ctry,trade_ctry,ctgr)\n",
    "\n",
    "    ### Our data is weekly based and the exploratory analysis has shown us that there is a clear seasonality. \n",
    "    ### So let's set up seasonal_order parameter to see if we improve the estimation and for train data (all observations except the last year)\n",
    "    df_prices_train, df_prices_test = \\\n",
    "        train_test_split(df_prices, shuffle=False, test_size=len(df_prices[df_prices.index.year==max(df_prices.index.year)]))\n",
    "\n",
    "    \n",
    "    # Evaluate parameters\n",
    "    p_values = range(0, 10)\n",
    "    d_values = range(0, 5)\n",
    "    q_values = range(0, 5)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Get the best SARIMA model\n",
    "    best_model = evaluate_models(df_prices_train.values, p_values, d_values, q_values, crop, ctry, dfNullID)\n",
    "\n",
    "    # Generate model!\n",
    "    if mdel == 'SARIMA':\n",
    "        model = SARIMAX(df_prices_train, order = best_model, seasonal_order=(1, 1, 1, 52)).fit()\n",
    "    elif mdel == 'ARIMA':\n",
    "        model = SARIMAX(df_prices_train, order = best_model).fit()\n",
    "\n",
    "    # Monkey patch around bug in ARIMA class\n",
    "    def __getnewargs__(self):\n",
    "        return ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "    ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "    # Save model as a pickle, .pkl file\n",
    "    model.save(f'../../data/03_models/model_{mdel_lc}_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}.pkl')\n",
    "\n",
    "    # Save model summary as an independent file\t\t\n",
    "    plt.rc('figure', figsize=(12, 7))\n",
    "    plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    updated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_img = f'../../data/03_models/Summary_{mdel}_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}_{updated}.png'\n",
    "    plt.savefig(dir_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarima_model_vols(crop,ctry,trade_ctry,ctgr):\n",
    "\n",
    "    ##  Function to train a ARIMA model and save it as a pickle .pkl file ## \n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import transformations as transf\n",
    "    import extractions as extract\n",
    "    import config\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    crop_lc = crop.lower()\n",
    "    ctry_lc = ctry.lower()\n",
    "    tctr_lc = trade_ctry.lower()\n",
    "    ctgr_lc = ctgr.lower()\n",
    "\n",
    "    # Get prices interpolated\n",
    "    df_vols = extract.get_volumes(crop,ctry,trade_ctry)\n",
    "\n",
    "    # Save null indexes with their original index id\n",
    "    dfNullID = extract.get_null_prices(crop,ctry,trade_ctry,ctgr)\n",
    "\n",
    "    ### Our data is weekly based and the exploratory analysis has shown us that there is a clear seasonality. \n",
    "    ### So let's set up seasonal_order parameter to see if we improve the estimation and for train data (all observations except the last year)\n",
    "    df_vols_train, df_vols_test = \\\n",
    "        train_test_split(df_vols, shuffle=False, test_size=len(df_vols[df_vols.index.year==max(df_vols.index.year)]))\n",
    "\n",
    "    # Evaluate parameters\n",
    "    p_values = range(0, 10)\n",
    "    d_values = range(0, 5)\n",
    "    q_values = range(0, 5)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Get the best ARIMA model\n",
    "    best_model = evaluate_models(df_vols_train.values, p_values, d_values, q_values, crop, ctry, dfNullID)\n",
    "\n",
    "    # Generate model!\n",
    "    model = SARIMAX(df_vols_train, order = best_model, seasonal_order=(1, 1, 1, 52)).fit()\n",
    "\n",
    "    # Monkey patch around bug in ARIMA class\n",
    "    def __getnewargs__(self):\n",
    "        return ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "    ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "    # Save model as a pickle, .pkl file\n",
    "    model.save(f'../../data/03_models/model_sarima_vols_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}.pkl')\n",
    "\n",
    "    # Save model summary as an independent file\t\t\n",
    "    plt.rc('figure', figsize=(12, 7))\n",
    "    plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    updated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_img = f'../../data/03_models/Summary_SARIMA_vols_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}_{updated}.png'\n",
    "    plt.savefig(dir_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_xmodels(dataset, exog, p_values, d_values, q_values, crop, ctry, dfNullID):\n",
    "\n",
    "    ##  Function to evaluate several combinations of p, d and q values in SARIMAX model and select the order with the least MAE ##\n",
    "\n",
    "\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\tfrom sklearn.metrics import mean_squared_error\n",
    "\tfrom sklearn.metrics import mean_absolute_error\n",
    "\timport warnings\n",
    "\timport pandas as pd\n",
    "\timport pyodbc\n",
    "\tfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\tfrom pandas import read_csv\n",
    "\tfrom pandas import datetime\n",
    "\tfrom statsmodels.tsa.arima_model import ARIMA\n",
    "\timport numpy as np\n",
    "\timport pmdarima as pm\n",
    "\tfrom pmdarima import model_selection\n",
    "\timport datetime\n",
    "\tfrom datetime import datetime, timedelta\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tdataset = dataset.astype('float32')\n",
    "\tbest_score, best_cfg = float(\"inf\"), None\n",
    "\tfor p in p_values:\n",
    "\t\tfor d in d_values:\n",
    "\t\t\tfor q in q_values:\n",
    "\t\t\t\torder = (p,d,q)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmae = evaluate_sarimax_model(dataset, exog, order, dfNullID)\n",
    "\t\t\t\t\tif mae < best_score:\n",
    "\t\t\t\t\t\tbest_score, best_cfg = mae, order\n",
    "\t\t\t\t\tprint('ARIMA%s MAE=%.3f' % (order,mae))\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\tprint('Best SARIMAX%s MAE=%.3f' % (best_cfg, best_score))\n",
    "\tversions_file = '../../data/03_models/Model_versions.txt'\n",
    "\tmodel_data = 'Best SARIMAX%s // MAE=%.3f // ' % (best_cfg, best_score)\n",
    "\tupdated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\twith open(versions_file, \"a\") as f:\n",
    "\t\tf.write(\"##\"+crop+\" \"+ctry+\" // \"+model_data+\"Updated \"+updated+\"##\\n\")\n",
    "\t\n",
    "\treturn(best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sarimax_model(X, exog, arima_order, dfNullID):\n",
    "\n",
    "    ##  Function to evaluate a SARIMAX model for a given order (p,d,q) ##\n",
    "\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Training dataset preparation\n",
    "    # Pick only campaign weeks for measure the prediction error\n",
    "    X_clean = pd.DataFrame(X)[~pd.DataFrame(X).index.isin(dfNullID['ID'])].values\n",
    "\n",
    "    # Endogenous feature\n",
    "    train_size = int(len(X_clean) * 0.66)\n",
    "    train, test = X_clean[0:train_size], X_clean[train_size:]\n",
    "    X_history = [x for x in train]\n",
    "\n",
    "    # Exogenous features\n",
    "    exog_clean = pd.DataFrame(exog)[~pd.DataFrame(exog).index.isin(dfNullID['ID'])].values.reshape(-1,1)\n",
    "    train_exog, test_exog = exog_clean[0:train_size], exog_clean[train_size:]\n",
    "    exog_history = [y for y in train_exog]\n",
    "\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model_fit = SARIMAX(X_history, exog=exog_history, order=arima_order, initialization='approximate_diffuse').fit() \n",
    "        # https://github.com/statsmodels/statsmodels/issues/5459#issuecomment-480562703\n",
    "        yhat = model_fit.predict()[0] #forecast\n",
    "        predictions.append(yhat)\n",
    "        X_history.append(test[t])\n",
    "        exog_history.append(test_exog[t])\n",
    "    # calculate out of sample error\n",
    "    error = mean_absolute_error(test, predictions) #MAE is the metric selected as price fluctuation could be up or down\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarimax_model(crop,ctry,trade_ctry,ctgr,exog):\n",
    "\n",
    "    ##  Function to train a SARIMAX model and save it as a pickle .pkl file ## \n",
    "\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    import extractions as extract\n",
    "    import transformations as transf\n",
    "\n",
    "    crop_lc = crop.lower()\n",
    "    ctry_lc = ctry.lower()\n",
    "    tctr_lc = trade_ctry.lower()\n",
    "    ctgr_lc = ctgr.lower()\n",
    "\n",
    "    endog = extract.get_prices_interpolated(crop,ctry,trade_ctry,ctgr)  # this dataframe contains all prices interpolated weekly and mean\n",
    "    dfNullID = extract.get_null_prices(crop,ctry,trade_ctry,ctgr)   # this dataframe contains the null indexes with their original index id\n",
    "\n",
    "    # Save exogenous dataframe with same shape as endogenous dataset for getting best model\n",
    "    exog = endog.join(exog.fillna(value=0)).fillna(value=0).drop('Price',axis=1)\n",
    "\n",
    "    ### Data is weekly based and the exploratory analysis has shown that there is a clear seasonality between campaigns (years)\n",
    "    ### So let's set up seasonal_order parameter to see if we improve the estimation and for train data (all observations except the last year)\n",
    "    endog_train, endog_test = train_test_split(endog, shuffle=False, test_size=len(endog[endog.index.year==max(endog.index.year)]))\n",
    "    exog_train, exog_test = train_test_split(exog, shuffle=False, test_size=len(exog[exog.index.year==max(exog.index.year)]))\n",
    "\n",
    "    # Normalization of prices (target variable)\n",
    "    endog_train_norm_inst, endog_train_norm = transf.normalize(endog_train)\n",
    "\n",
    "    endog_train_norm = pd.DataFrame(endog_train_norm)\n",
    "    endog_train_norm.index = endog_train.index\n",
    "    endog_train_norm.columns = endog_train.columns.values\n",
    "\n",
    "    # Normalization of exogenous variables\n",
    "    exog_train_norm_inst, exog_train_norm = transf.normalize(exog_train)\n",
    "\n",
    "    exog_train_norm = pd.DataFrame(exog_train_norm)\n",
    "    exog_train_norm.index = exog_train.index\n",
    "    exog_train_norm.columns = exog_train.columns.values\n",
    "    \n",
    "    # Evaluate parameters\n",
    "    p_values = range(0, 10)\n",
    "    d_values = range(0, 5)\n",
    "    q_values = range(0, 5)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Get the best SARIMAX model\n",
    "    best_model = evaluate_xmodels(endog_train_norm, exog_train_norm, p_values, d_values, q_values, crop, ctry, dfNullID)\n",
    "\n",
    "    # Generate model!\n",
    "    model = SARIMAX(endog_train_norm, exog = exog_train_norm, order = best_model, seasonal_order=(1, 1, 1, 52)).fit()\n",
    "\n",
    "    # Monkey patch around bug in ARIMA class\n",
    "    def __getnewargs__(self):\n",
    "        return ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "    ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "    # Save model as a pickle, .pkl file\n",
    "    model.save(f'../../data/03_models/model_sarimax_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}.pkl')\n",
    "\n",
    "    # Save model summary as an independent file\t\t\n",
    "    plt.rc('figure', figsize=(12, 7))\n",
    "    plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    updated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_img = f'../../data/03_models/Summary_SARIMAX_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}_{updated}.png'\n",
    "    plt.savefig(dir_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_measures(crop_list):\n",
    "\n",
    "    ##  Function to calculate measures of the different models .pkl saved ##\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    #   https://realpython.com/python-modules-packages/\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import extractions as extract\n",
    "    import transformations as transf\n",
    "    import training as train\n",
    "    import import_data as imp\n",
    "    import inference as inf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import config\n",
    "    import time  \n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from datetime import date\n",
    "    import config as conf\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    from statsmodels.tsa.arima_model import ARIMAResults\n",
    "    from datetime import datetime, timedelta\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAXResults\n",
    "    import pyodbc\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    df_all_results = pd.DataFrame()\n",
    "\n",
    "    # Iter through all crop models of crop_list\n",
    "    for i in range(0, len(crop_list)):\n",
    "\n",
    "        crop = crop_list[i][0]\n",
    "        ctry = crop_list[i][1]\n",
    "        trade_ctry = crop_list[i][2]\n",
    "        ctgr = crop_list[i][3]\n",
    "        mdel = crop_list[i][4]\n",
    "        regn = crop_list[i][5]\n",
    "        pkge = crop_list[i][6]\n",
    "        crcy = crop_list[i][7]\n",
    "        msre = crop_list[i][8]\n",
    "        exog = crop_list[i][9]\n",
    "\n",
    "        ctry_lc = ctry.lower()\n",
    "        crop_lc = crop.lower()\n",
    "        mdel_lc = mdel.lower()\n",
    "        trade_ctry_lc = trade_ctry.lower()\n",
    "        ctgr_lc = ctgr.lower()\n",
    "\n",
    "        model_name = f'../../data/03_models/model_{mdel_lc}_{crop_lc}_{ctry_lc}_{trade_ctry_lc}_{ctgr_lc}.pkl'\n",
    "\n",
    "        try:\n",
    "            ld_model = ARIMAResults.load(model_name)\n",
    "        except FileNotFoundError:\n",
    "            print('No model found')\n",
    "            break\n",
    "\n",
    "        # FIRST save standard results obtained from model summary:\n",
    "     \n",
    "        results_summary = ld_model.summary()\n",
    "        # Note that tables is a list. The table at index 1 is the \"core\" table. Additionally, read_html puts dfs in a list, so we want index 0\n",
    "        results_as_html = results_summary.tables[0].as_html()\n",
    "        df_results_int = pd.read_html(results_as_html, index_col=0)[0]\n",
    "        df_results_int['Crop'] = crop\n",
    "        df_results_int['Country'] = ctry\n",
    "        df_results_int['Trade_Country']  = trade_ctry\n",
    "        df_results_int['Model'] = mdel\n",
    "        df_results_int['Category']  = ctgr\n",
    "        df_results = df_results_int.reset_index().iloc[:, 0:2].rename(columns={0: \"Concept\", 1: \"Result\"})\n",
    "        df_results = df_results.append(df_results_int.reset_index().iloc[:, 2:4].rename(columns={2: \"Concept\", 3: \"Result\"})).dropna()\n",
    "        df_results = df_results.join(df_results_int.reset_index().iloc[:, 4:])\n",
    "        df_all_results = df_all_results.append(df_results)\n",
    "\n",
    "        # SECOND save calculated measures. \n",
    "        # All models are fitted taking training values up to last day of previous year and inferenced the prediction for the next two years\n",
    "        # Taking into account this MAE, MAPE, MSE and RMSE are measured\n",
    "        start = date.today().strftime('%Y-01-01')\n",
    "        end = (date.today() + relativedelta(years=1)).strftime('%Y-12-31')\n",
    "        \n",
    "        if mdel == 'SARIMAX':\n",
    "            mdel_vols = 'SARIMA'\n",
    "            df_pred_vols = inf.get_prediction_vols(ctry,crop,trade_ctry,regn,ctgr,pkge,crcy,msre,mdel_vols,start,end)\n",
    "            exog = df_pred_vols[df_pred_vols.Date_ref > date.today().strftime('%Y-01-01')].drop(columns=['Volume']).set_index('Date_ref')\n",
    "\n",
    "        df_pred = inf.get_prediction(ctry,crop,trade_ctry,regn,ctgr,pkge,crcy,msre,mdel,exog,start,end)\n",
    "\n",
    "        df_pred = df_pred[(df_pred['Date_ref'].dt.year == date.today().year) & (df_pred['Date_ref'] < datetime.today()) & (df_pred['Price_estimated'] != 0) & (df_pred['Price'] != 0)]\n",
    "\n",
    "        # MAE\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        mae = mean_absolute_error(df_pred.Price,df_pred.Price_estimated)\n",
    "        new_row = {'Concept':'MAE', 'Result':mae, 'Crop':crop, 'Country':ctry, 'Trade_Country':trade_ctry, 'Model':mdel, 'Category':ctgr}\n",
    "        df_all_results = df_all_results.append(new_row, ignore_index=True)\n",
    "\n",
    "        # MAPE\n",
    "        mape = np.mean(np.abs(df_pred.Price-df_pred.Price_estimated)/df_pred.Price_estimated)\n",
    "        new_row = {'Concept':'MAPE', 'Result':mape, 'Crop':crop, 'Country':ctry, 'Trade_Country':trade_ctry, 'Model':mdel, 'Category':ctgr}\n",
    "        df_all_results = df_all_results.append(new_row, ignore_index=True)\n",
    "\n",
    "        # MSE\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        # Use against predictions (we must calculate the square root of the MSE)\n",
    "        mse = mean_squared_error(df_pred.Price,df_pred.Price_estimated)\n",
    "        new_row = {'Concept':'MSE', 'Result':mse, 'Crop':crop, 'Country':ctry, 'Trade_Country':trade_ctry, 'Model':mdel, 'Category':ctgr}\n",
    "        df_all_results = df_all_results.append(new_row, ignore_index=True)\n",
    "\n",
    "        # RMSE\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        # Use against predictions (we must calculate the square root of the MSE)\n",
    "        rmse = np.sqrt(mean_squared_error(df_pred.Price,df_pred.Price_estimated))\n",
    "        new_row = {'Concept':'RMSE', 'Result':rmse, 'Crop':crop, 'Country':ctry, 'Trade_Country':trade_ctry, 'Model':mdel, 'Category':ctgr}\n",
    "        df_all_results = df_all_results.append(new_row, ignore_index=True)\n",
    "\n",
    "        df_all_results['Result_num'] = df_all_results[df_all_results.Concept.isin(['AIC','BIC','HQIC','MAE','MAPE','MSE','RMSE'])].Result.apply(pd.to_numeric, errors='coerce')\n",
    "        df_all_results['Result_num'].fillna(0, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_all_results.to_excel('../../data/03_models/results_summary.xlsx')\n",
    "\n",
    "    return df_all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_measures_db(df_all_results):\n",
    "\n",
    "    ##  Function to save model measures results into the database ## \n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    #   https://realpython.com/python-modules-packages/\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import extractions as extract\n",
    "    import transformations as transf\n",
    "    import training as train\n",
    "    import import_data as imp\n",
    "    import inference as inf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import config\n",
    "    import time  \n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from datetime import date\n",
    "    import config as conf\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    from statsmodels.tsa.arima_model import ARIMAResults\n",
    "    from datetime import datetime, timedelta\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAXResults\n",
    "    import pyodbc\n",
    "\n",
    "    connStr = pyodbc.connect(config.db_con)\n",
    "    cursor = connStr.cursor()\n",
    "\n",
    "    # Load all data\n",
    "    upd = 0\n",
    "\n",
    "    for index,row in df_all_results.iterrows():\n",
    "        if row['Result_num'] != 0: # Save only valid measures, not descriptive data\n",
    "            cursor.execute(\"INSERT INTO dbo.models([Model],[Product],[Country],[Trade_Country],[Category],[Concept],[Result],[Updated]) values (?,?,?,?,?,?,?,?)\",row['Model'],row['Crop'],row['Country'],row['Trade_Country'],row['Category'],row['Concept'],row['Result_num'],datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            connStr.commit()\n",
    "            upd += 1\n",
    "\n",
    "    cursor.close()\n",
    "    connStr.close()\n",
    "\n",
    "    return (print(upd,\" new prices added\"))"
   ]
  }
 ]
}