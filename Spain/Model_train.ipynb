{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First model --> https://www.youtube.com/watch?v=WjeGUs6mzXg\n",
    "# https://machinelearningmastery.com/grid-search-arima-hyperparameters-with-python/\n",
    "\n",
    "# evaluate an ARIMA model for a given order (p,d,q)\n",
    "def evaluate_arima_model(X, arima_order,dfNullID):\n",
    "\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\tfrom sklearn.metrics import mean_squared_error\n",
    "\tfrom sklearn.metrics import mean_absolute_error\n",
    "\timport warnings\n",
    "\timport pandas as pd\n",
    "\timport pyodbc\n",
    "\tfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\tfrom pandas import read_csv\n",
    "\tfrom pandas import datetime\n",
    "\tfrom statsmodels.tsa.arima_model import ARIMA\n",
    "\timport numpy as np\n",
    "\timport pmdarima as pm\n",
    "\tfrom pmdarima import model_selection\n",
    "\timport datetime\n",
    "\tfrom datetime import datetime, timedelta\n",
    "\timport matplotlib.pyplot as plt\n",
    "\n",
    "\t# prepare training dataset\n",
    "\tX_clean = pd.DataFrame(X)[~pd.DataFrame(X).index.isin(dfNullID['ID'])].values # Pick only campaign weeks for measure the prediction error\n",
    "\ttrain_size = int(len(X_clean) * 0.66)\n",
    "\ttrain, test = X_clean[0:train_size], X_clean[train_size:]\n",
    "\thistory = [x for x in train]\n",
    "\t# make predictions\n",
    "\tpredictions = list()\n",
    "\tfor t in range(len(test)):\n",
    "\t\tmodel_fit = SARIMAX(history, order=arima_order).fit()\n",
    "\t\t#model_fit = model.fit(disp=0)\n",
    "\t\tyhat = model_fit.forecast()[0]\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\thistory.append(test[t])\n",
    "\t# calculate out of sample error\n",
    "\terror = mean_absolute_error(test, predictions) #MAE is the metric selected as price fluctuation could be up or down\n",
    "\n",
    "\treturn error\n",
    "\n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values, crop, ctry, dfNullID):\n",
    "\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\tfrom sklearn.metrics import mean_squared_error\n",
    "\tfrom sklearn.metrics import mean_absolute_error\n",
    "\timport warnings\n",
    "\timport pandas as pd\n",
    "\timport pyodbc\n",
    "\tfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\tfrom pandas import read_csv\n",
    "\tfrom pandas import datetime\n",
    "\tfrom statsmodels.tsa.arima_model import ARIMA\n",
    "\timport numpy as np\n",
    "\timport pmdarima as pm\n",
    "\tfrom pmdarima import model_selection\n",
    "\timport datetime\n",
    "\tfrom datetime import datetime, timedelta\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tdataset = dataset.astype('float32')\n",
    "\tbest_score, best_cfg = float(\"inf\"), None\n",
    "\tfor p in p_values:\n",
    "\t\tfor d in d_values:\n",
    "\t\t\tfor q in q_values:\n",
    "\t\t\t\torder = (p,d,q)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmae = evaluate_arima_model(dataset, order, dfNullID)\n",
    "\t\t\t\t\tif mae < best_score:\n",
    "\t\t\t\t\t\tbest_score, best_cfg = mae, order\n",
    "\t\t\t\t\tprint('ARIMA%s MAE=%.3f' % (order,mae))\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\tprint('Best ARIMA%s MAE=%.3f' % (best_cfg, best_score))\n",
    "\tversions_file = './Model/Model_versions.txt'\n",
    "\tmodel_data = 'Best ARIMA%s // MAE=%.3f // ' % (best_cfg, best_score)\n",
    "\tupdated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\twith open(versions_file, \"a\") as f:\n",
    "\t\tf.write(\"##\"+crop+\" \"+ctry+\" // \"+model_data+\"Updated \"+updated+\"##\\n\")\n",
    "\t\n",
    "\treturn(best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_arima_model(crop,ctry):\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    crop_lc = crop.lower()\n",
    "    ctry_lc = ctry.lower()\n",
    "\n",
    "    connStr = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER=bipro02\\\\adminbi;DATABASE=Prices;Trusted_Connection=yes')\n",
    "    cursor = connStr.cursor()\n",
    "\n",
    "    qry = f\"SELECT * FROM [Prices].[dbo].[prices] where cast([Country] as nvarchar) = cast('{ctry}' as nvarchar) and cast([Product] as nvarchar) = cast('{crop}' as nvarchar)\"\n",
    "    df_prices = pd.read_sql(qry, connStr)\n",
    "\n",
    "    df_prices = df_prices[df_prices.Campaign > min(df_prices.Campaign)][['Date_price', 'Price']]\n",
    "    df_prices.set_index('Date_price',inplace=True)\n",
    "    df_prices.sort_index(inplace=True)\n",
    "    df_prices.index = df_prices.index.astype('datetime64[ns]') \n",
    "    df_prices = df_prices.resample('W-MON').mean()\n",
    "    rows_null = df_prices.isnull()\n",
    "    idx_null = rows_null[rows_null.any(axis=1)].index\n",
    "    df_prices_all = df_prices.interpolate()\n",
    "    df_prices_non_zero = df_prices_all[~df_prices_all.index.isin(idx_null)]\n",
    "    listIndex = list(zip(df_prices_all.index, range(0,len(df_prices_all))))     # save all indexes in tuples list (index, idPosition)\n",
    "    listNull = idx_null     # save all null indexes\n",
    "\n",
    "    dfIndex = pd.DataFrame(listIndex)\n",
    "    dfNull = pd.DataFrame(listNull)\n",
    "    dfIndex.columns = ['Date_price','ID']\n",
    "    dfNullID = dfIndex.merge(dfNull, how='inner', on='Date_price')    # this dataframe contains the null indexes with their original index id\n",
    "\n",
    "    # Evaluate parameters\n",
    "    p_values = range(0, 10)\n",
    "    d_values = range(0, 5)\n",
    "    q_values = range(0, 5)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    best_model = evaluate_models(df_prices_all.values, p_values, d_values, q_values, crop, ctry, dfNullID)\n",
    "\n",
    "    ### Our data is weekly based and the exploratory analysis has shown us that there is a clear seasonality. \n",
    "    ### So let's set up seasonal_order parameter to see if we improve the estimation and for train data (all observations except the last year)\n",
    "    df_prices_all_train, df_prices_all_test = \\\n",
    "        train_test_split(df_prices_all, shuffle=False, test_size=len(df_prices_all[df_prices_all.index.year==max(df_prices_all.index.year)]))\n",
    "    model = SARIMAX(df_prices_all_train, order = best_model, seasonal_order=(1, 1, 1, 52)).fit()\n",
    "\n",
    "    # SAVE MODEL\n",
    "    # monkey patch around bug in ARIMA class\n",
    "    def __getnewargs__(self):\n",
    "        return ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "    ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "    # save model\n",
    "    model.save(f'Model/model_arima_{crop_lc}_{ctry_lc}.pkl')\n",
    "\n",
    "    # save model info\t\t\n",
    "    plt.rc('figure', figsize=(12, 7))\n",
    "    plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    updated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_img = f'Model/Summary_{crop_lc}_{ctry_lc}_{updated}.png'\n",
    "    plt.savefig(dir_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_sarimax_model(crop,ctry,trade_ctry):\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    crop = 'BLUEBERRIES'\n",
    "    ctry = 'EU'\n",
    "    trade_ctry = 'ES'\n",
    "\n",
    "    # Obtaining exogenous variable for SARIMAX model\n",
    "\n",
    "    # UE volumes import from Spain \n",
    "    connStr = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER=bipro02\\\\adminbi;DATABASE=Prices;Trusted_Connection=yes')\n",
    "    cursor = connStr.cursor()\n",
    "\n",
    "    qry = f\"SELECT * FROM [Prices].[dbo].[volumes] where cast([Country] as nvarchar) = cast('{ctry}' as nvarchar) and cast([Product] as nvarchar) = cast('{crop}' as nvarchar) and cast([Trade_Country] as nvarchar) = cast('{trade_ctry}' as nvarchar)\"\n",
    "\n",
    "    df_volumes = pd.read_sql(qry, connStr)\n",
    "\n",
    "    df_volumes = df_volumes[df_volumes.Campaign > min(df_volumes.Campaign)][['Date_volume', 'Volume']]\n",
    "    df_volumes.groupby('Date_volume').agg('sum')\n",
    "    df_volumes.set_index('Date_volume',inplace=True)\n",
    "    df_volumes.sort_index(inplace=True)\n",
    "    df_volumes.index = df_volumes.index.astype('datetime64[ns]') \n",
    "    df_volumes = df_volumes.resample('W-MON').sum()\n",
    "\n",
    "    # Labor Cost index evolution in Spain\n",
    "\n",
    "    # Need to lag 1 year in order to allocate campaign costs to the actual fresh produce sales\n",
    "    df_salaries = pd.read_excel('./Data/LaborCostIndex')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit5c446d3ae10d4c4b9ddac51de7b948b5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}