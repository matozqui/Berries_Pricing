{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599145821351",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_arima_model(X, arima_order,dfNullID):\n",
    "\n",
    "    ##  Function to evaluate a ARIMA model for a given order (p,d,q) ##\n",
    "\n",
    "\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\tfrom sklearn.metrics import mean_squared_error\n",
    "\tfrom sklearn.metrics import mean_absolute_error\n",
    "\timport warnings\n",
    "\timport pandas as pd\n",
    "\timport pyodbc\n",
    "\tfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\tfrom pandas import read_csv\n",
    "\tfrom pandas import datetime\n",
    "\tfrom statsmodels.tsa.arima_model import ARIMA\n",
    "\timport numpy as np\n",
    "\timport pmdarima as pm\n",
    "\tfrom pmdarima import model_selection\n",
    "\timport datetime\n",
    "\tfrom datetime import datetime, timedelta\n",
    "\timport matplotlib.pyplot as plt\n",
    "\n",
    "\t# prepare training dataset\n",
    "\tX_clean = pd.DataFrame(X)[~pd.DataFrame(X).index.isin(dfNullID['ID'])].values # Pick only campaign weeks for measure the prediction error\n",
    "\ttrain_size = int(len(X_clean) * 0.66)\n",
    "\ttrain, test = X_clean[0:train_size], X_clean[train_size:]\n",
    "\thistory = [x for x in train]\n",
    "\t# make predictions\n",
    "\tpredictions = list()\n",
    "\tfor t in range(len(test)):\n",
    "\t\tmodel_fit = SARIMAX(history, order=arima_order).fit()\n",
    "\t\t#model_fit = model.fit(disp=0)\n",
    "\t\tyhat = model_fit.forecast()[0]\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\thistory.append(test[t])\n",
    "\t# calculate out of sample error\n",
    "\terror = mean_absolute_error(test, predictions) #MAE is the metric selected as price fluctuation could be up or down\n",
    "\n",
    "\treturn error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(dataset, p_values, d_values, q_values, crop, ctry, dfNullID):\n",
    "\n",
    "    ##  Function to evaluate several combinations of p, d and q values in ARIMA model and select the order with the least MAE ##\n",
    "\n",
    "\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\tfrom sklearn.metrics import mean_squared_error\n",
    "\tfrom sklearn.metrics import mean_absolute_error\n",
    "\timport warnings\n",
    "\timport pandas as pd\n",
    "\timport pyodbc\n",
    "\tfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\tfrom pandas import read_csv\n",
    "\tfrom pandas import datetime\n",
    "\tfrom statsmodels.tsa.arima_model import ARIMA\n",
    "\timport numpy as np\n",
    "\timport pmdarima as pm\n",
    "\tfrom pmdarima import model_selection\n",
    "\timport datetime\n",
    "\tfrom datetime import datetime, timedelta\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tdataset = dataset.astype('float32')\n",
    "\tbest_score, best_cfg = float(\"inf\"), None\n",
    "\tfor p in p_values:\n",
    "\t\tfor d in d_values:\n",
    "\t\t\tfor q in q_values:\n",
    "\t\t\t\torder = (p,d,q)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmae = evaluate_arima_model(dataset, order, dfNullID)\n",
    "\t\t\t\t\tif mae < best_score:\n",
    "\t\t\t\t\t\tbest_score, best_cfg = mae, order\n",
    "\t\t\t\t\tprint('ARIMA%s MAE=%.3f' % (order,mae))\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\tprint('Best ARIMA%s MAE=%.3f' % (best_cfg, best_score))\n",
    "\tversions_file = '../../data/04_models/Model_versions.txt'\n",
    "\tmodel_data = 'Best ARIMA%s // MAE=%.3f // ' % (best_cfg, best_score)\n",
    "\tupdated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\twith open(versions_file, \"a\") as f:\n",
    "\t\tf.write(\"##\"+crop+\" \"+ctry+\" // \"+model_data+\"Updated \"+updated+\"##\\n\")\n",
    "\t\n",
    "\treturn(best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_arima_model(crop,ctry,trade_ctry,ctgr):\n",
    "\n",
    "    ##  Function to train a ARIMA model and save it as a pickle .pkl file ## \n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import transformations as transf\n",
    "    import extractions as extract\n",
    "    import config\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    crop_lc = crop.lower()\n",
    "    ctry_lc = ctry.lower()\n",
    "    tctr_lc = trade_ctry.lower()\n",
    "    ctgr_lc = ctgr.lower()\n",
    "\n",
    "    # Get prices interpolated\n",
    "    df_prices = extract.get_prices_interpolated(crop,ctry,trade_ctry,ctgr)\n",
    "\n",
    "    # Save null indexes with their original index id\n",
    "    dfNullID = extract.get_null_prices(crop,ctry,trade_ctry,ctgr)\n",
    "\n",
    "    ### Our data is weekly based and the exploratory analysis has shown us that there is a clear seasonality. \n",
    "    ### So let's set up seasonal_order parameter to see if we improve the estimation and for train data (all observations except the last year)\n",
    "    df_prices_train, df_prices_test = \\\n",
    "        train_test_split(df_prices, shuffle=False, test_size=len(df_prices[df_prices.index.year==max(df_prices.index.year)]))\n",
    "\n",
    "    \n",
    "    # Evaluate parameters\n",
    "    p_values = range(0, 10)\n",
    "    d_values = range(0, 5)\n",
    "    q_values = range(0, 5)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Get the best ARIMA model\n",
    "    best_model = evaluate_models(df_prices_train.values, p_values, d_values, q_values, crop, ctry, dfNullID)\n",
    "\n",
    "    # Generate model!\n",
    "    model = SARIMAX(df_prices_train, order = best_model, seasonal_order=(1, 1, 1, 52)).fit()\n",
    "\n",
    "    # Monkey patch around bug in ARIMA class\n",
    "    def __getnewargs__(self):\n",
    "        return ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "    ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "    # Save model as a pickle, .pkl file\n",
    "    model.save(f'../../data/04_models/model_arima_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}.pkl')\n",
    "\n",
    "    # Save model summary as an independent file\t\t\n",
    "    plt.rc('figure', figsize=(12, 7))\n",
    "    plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    updated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_img = f'../../data/04_models/Summary_ARIMA_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}_{updated}.png'\n",
    "    plt.savefig(dir_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sarimax_model(X, exog, arima_order, dfNullID):\n",
    "\n",
    "    ##  Function to evaluate a SARIMAX model for a given order (p,d,q) ##\n",
    "\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Training dataset preparation\n",
    "    # Pick only campaign weeks for measure the prediction error\n",
    "    X_clean = pd.DataFrame(X)[~pd.DataFrame(X).index.isin(dfNullID['ID'])].values\n",
    "\n",
    "    # Endogenous feature\n",
    "    train_size = int(len(X_clean) * 0.66)\n",
    "    train, test = X_clean[0:train_size], X_clean[train_size:]\n",
    "    X_history = [x for x in train]\n",
    "\n",
    "    # Exogenous features\n",
    "    exog_clean = pd.DataFrame(exog)[~pd.DataFrame(exog).index.isin(dfNullID['ID'])].values.reshape(-1,1)\n",
    "    train_exog, test_exog = exog_clean[0:train_size], exog_clean[train_size:]\n",
    "    exog_history = [y for y in train_exog]\n",
    "\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model_fit = SARIMAX(X_history, exog=exog_history, order=arima_order, initialization='approximate_diffuse').fit() \n",
    "        # https://github.com/statsmodels/statsmodels/issues/5459#issuecomment-480562703\n",
    "        yhat = model_fit.predict()[0] #forecast\n",
    "        predictions.append(yhat)\n",
    "        X_history.append(test[t])\n",
    "        exog_history.append(test_exog[t])\n",
    "    # calculate out of sample error\n",
    "    error = mean_absolute_error(test, predictions) #MAE is the metric selected as price fluctuation could be up or down\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_xmodels(dataset, exog, p_values, d_values, q_values, crop, ctry, dfNullID):\n",
    "\n",
    "    ##  Function to evaluate several combinations of p, d and q values in SARIMAX model and select the order with the least MAE ##\n",
    "\n",
    "\tfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\tfrom sklearn.metrics import mean_squared_error\n",
    "\tfrom sklearn.metrics import mean_absolute_error\n",
    "\timport warnings\n",
    "\timport pandas as pd\n",
    "\timport pyodbc\n",
    "\tfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\tfrom pandas import read_csv\n",
    "\tfrom pandas import datetime\n",
    "\tfrom statsmodels.tsa.arima_model import ARIMA\n",
    "\timport numpy as np\n",
    "\timport pmdarima as pm\n",
    "\tfrom pmdarima import model_selection\n",
    "\timport datetime\n",
    "\tfrom datetime import datetime, timedelta\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tdataset = dataset.astype('float32')\n",
    "\tbest_score, best_cfg = float(\"inf\"), None\n",
    "\tfor p in p_values:\n",
    "\t\tfor d in d_values:\n",
    "\t\t\tfor q in q_values:\n",
    "\t\t\t\torder = (p,d,q)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tmae = evaluate_sarimax_model(dataset, exog, order, dfNullID)\n",
    "\t\t\t\t\tif mae < best_score:\n",
    "\t\t\t\t\t\tbest_score, best_cfg = mae, order\n",
    "\t\t\t\t\tprint('ARIMA%s MAE=%.3f' % (order,mae))\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\tprint('Best SARIMAX%s MAE=%.3f' % (best_cfg, best_score))\n",
    "\tversions_file = '../../data/04_models/Model_versions.txt'\n",
    "\tmodel_data = 'Best SARIMAX%s // MAE=%.3f // ' % (best_cfg, best_score)\n",
    "\tupdated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\twith open(versions_file, \"a\") as f:\n",
    "\t\tf.write(\"##\"+crop+\" \"+ctry+\" // \"+model_data+\"Updated \"+updated+\"##\\n\")\n",
    "\t\n",
    "\treturn(best_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarimax_model(crop,ctry,trade_ctry,ctgr,exog):\n",
    "\n",
    "    ##  Function to train a SARIMAX model and save it as a pickle .pkl file ## \n",
    "\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    import extractions as extract\n",
    "    import transformations as transf\n",
    "\n",
    "    crop_lc = crop.lower()\n",
    "    ctry_lc = ctry.lower()\n",
    "    tctr_lc = trade_ctry.lower()\n",
    "    ctgr_lc = ctgr.lower()\n",
    "\n",
    "    endog = extract.get_prices_interpolated(crop,ctry,trade_ctry,ctgr)  # this dataframe contains all prices interpolated weekly and mean\n",
    "    dfNullID = extract.get_null_prices(crop,ctry,trade_ctry,ctgr)   # this dataframe contains the null indexes with their original index id\n",
    "\n",
    "    # Save exogenous dataframe with same shape as endogenous dataset for getting best model\n",
    "    exog = endog.join(exog.fillna(value=0)).fillna(value=0).drop('Price',axis=1)\n",
    "\n",
    "    ### Data is weekly based and the exploratory analysis has shown that there is a clear seasonality between campaigns (years)\n",
    "    ### So let's set up seasonal_order parameter to see if we improve the estimation and for train data (all observations except the last year)\n",
    "    endog_train, endog_test = train_test_split(endog, shuffle=False, test_size=len(endog[endog.index.year==max(endog.index.year)]))\n",
    "    exog_train, exog_test = train_test_split(exog, shuffle=False, test_size=len(exog[exog.index.year==max(exog.index.year)]))\n",
    "\n",
    "    # Normalization of prices (target variable)\n",
    "    endog_train_norm_inst, endog_train_norm = transf.normalize(endog_train)\n",
    "\n",
    "    endog_train_norm = pd.DataFrame(endog_train_norm)\n",
    "    endog_train_norm.index = endog_train.index\n",
    "    endog_train_norm.columns = endog_train.columns.values\n",
    "\n",
    "    # Normalization of exogenous variables\n",
    "    exog_train_norm_inst, exog_train_norm = transf.normalize(exog_train)\n",
    "\n",
    "    exog_train_norm = pd.DataFrame(exog_train_norm)\n",
    "    exog_train_norm.index = exog_train.index\n",
    "    exog_train_norm.columns = exog_train.columns.values\n",
    "    \n",
    "    # Evaluate parameters\n",
    "    p_values = range(0, 10)\n",
    "    d_values = range(0, 5)\n",
    "    q_values = range(0, 5)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Get the best SARIMAX model\n",
    "    best_model = evaluate_xmodels(endog_train_norm, exog_train_norm, p_values, d_values, q_values, crop, ctry, dfNullID)\n",
    "\n",
    "    # Generate model!\n",
    "    model = SARIMAX(endog_train_norm, exog = exog_train_norm, order = best_model, seasonal_order=(1, 1, 1, 52)).fit()\n",
    "\n",
    "    # Monkey patch around bug in ARIMA class\n",
    "    def __getnewargs__(self):\n",
    "        return ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "    ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "    # Save model as a pickle, .pkl file\n",
    "    model.save(f'../../data/04_models/model_sarimax_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}.pkl')\n",
    "\n",
    "    # Save model summary as an independent file\t\t\n",
    "    plt.rc('figure', figsize=(12, 7))\n",
    "    plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    updated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_img = f'../../data/04_models/Summary_SARIMAX_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}_{updated}.png'\n",
    "    plt.savefig(dir_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_arima_model_vols(crop,ctry,trade_ctry,ctgr):\n",
    "\n",
    "    ##  Function to train a ARIMA model and save it as a pickle .pkl file ## \n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import transformations as transf\n",
    "    import extractions as extract\n",
    "    import config\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    import pandas as pd\n",
    "    import pyodbc\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    import numpy as np\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    import datetime\n",
    "    from datetime import datetime, timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    crop_lc = crop.lower()\n",
    "    ctry_lc = ctry.lower()\n",
    "    tctr_lc = trade_ctry.lower()\n",
    "    ctgr_lc = ctgr.lower()\n",
    "\n",
    "    # Get prices interpolated\n",
    "    df_vols = extract.get_volumes(crop,ctry,trade_ctry)\n",
    "\n",
    "    # Save null indexes with their original index id\n",
    "    dfNullID = extract.get_null_prices(crop,ctry,trade_ctry,ctgr)\n",
    "\n",
    "    ### Our data is weekly based and the exploratory analysis has shown us that there is a clear seasonality. \n",
    "    ### So let's set up seasonal_order parameter to see if we improve the estimation and for train data (all observations except the last year)\n",
    "    df_vols_train, df_vols_test = \\\n",
    "        train_test_split(df_vols, shuffle=False, test_size=len(df_vols[df_vols.index.year==max(df_vols.index.year)]))\n",
    "\n",
    "    # Evaluate parameters\n",
    "    p_values = range(0, 10)\n",
    "    d_values = range(0, 5)\n",
    "    q_values = range(0, 5)\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Get the best ARIMA model\n",
    "    best_model = evaluate_models(df_vols_train.values, p_values, d_values, q_values, crop, ctry, dfNullID)\n",
    "\n",
    "    # Generate model!\n",
    "    model = SARIMAX(df_vols_train, order = best_model, seasonal_order=(1, 1, 1, 52)).fit()\n",
    "\n",
    "    # Monkey patch around bug in ARIMA class\n",
    "    def __getnewargs__(self):\n",
    "        return ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "    ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "    # Save model as a pickle, .pkl file\n",
    "    model.save(f'../../data/04_models/model_arima_vols_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}.pkl')\n",
    "\n",
    "    # Save model summary as an independent file\t\t\n",
    "    plt.rc('figure', figsize=(12, 7))\n",
    "    plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    updated = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dir_img = f'../../data/04_models/Summary_ARIMA_vols_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}_{updated}.png'\n",
    "    plt.savefig(dir_img)"
   ]
  }
 ]
}