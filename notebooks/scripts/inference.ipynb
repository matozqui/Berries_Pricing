{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(ctry,crop,trade_ctry,regn,ctgr,pkge,crcy,msre,mdel,exog,start,end):\n",
    "\n",
    "    ##  Function to call a model and make price predictions   ##\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import transformations as transf\n",
    "    import extractions as extract\n",
    "    import config\n",
    "    import pyodbc\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    from datetime import datetime, timedelta\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    from statsmodels.tsa.arima_model import ARIMAResults\n",
    "    from datetime import datetime, timedelta\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    df_prices = extract.get_prices_interpolated(crop,ctry,trade_ctry,ctgr)\n",
    "    dfNullID = extract.get_null_prices(crop,ctry,trade_ctry,ctgr)\n",
    "    df_prices_non_zero = df_prices[~df_prices.index.isin(dfNullID.set_index('Date_price').index)]\n",
    "\n",
    "    # load model\n",
    "    ctry_lc = ctry.lower()\n",
    "    crop_lc = crop.lower()\n",
    "    mdel_lc = mdel.lower()\n",
    "    tctr_lc = trade_ctry.lower()\n",
    "    ctgr_lc = ctgr.lower()\n",
    "\n",
    "    model_name = f'../../data/03_models/model_{mdel_lc}_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}.pkl'\n",
    "\n",
    "    try:\n",
    "        ld_model = ARIMAResults.load(model_name)\n",
    "    except FileNotFoundError:\n",
    "        print('No model found')\n",
    "        return\n",
    "        \n",
    "    # make predictions for last year and the following\n",
    "    yr = str(datetime.now().year+1)\n",
    "\n",
    "    #pred = ld_model.get_prediction(start=start, end=end, dynamic=True)\n",
    "    if mdel in ['ARIMA','SARIMA']:  \n",
    "        pred = ld_model.get_prediction(start=start, end=end, dynamic=True)\n",
    "        df_pred = pred.predicted_mean.to_frame(name='Price_estimated')\n",
    "    elif mdel == 'SARIMAX': \n",
    "        # Getting transformer for endogenous data\n",
    "        sc_out, df = transf.normalize(df_prices_non_zero.Price.to_frame(name='Price'))\n",
    "        # Normalization of exog data\n",
    "        sc_in_exog, exog = transf.normalize(exog)\n",
    "        # Model prediction\n",
    "        pred = ld_model.get_prediction(start=start, end=end, exog=exog, dynamic=True)\n",
    "        # Denormalization\n",
    "        df_pred = pred.predicted_mean.to_frame(name='Price_estimated')\n",
    "        descaling_input =transf.denormalize(sc_out,df_pred)\n",
    "        df_pred = descaling_input\n",
    "    else: \n",
    "        print(\"Invalid model\") \n",
    "        return\n",
    "    \n",
    "    # Generate only non-zero prices for predicted weeks, based on last-year-no-price weeks \n",
    "    last_yr_wk_zero = dfNullID[dfNullID['Date_price'].dt.year==int(yr)-2]['Date_price'].dt.week\n",
    "    df_pred = df_pred[~df_pred.index.week.isin(last_yr_wk_zero)]\n",
    "    \n",
    "    # Mondays and not Sundays as starting day of the week\n",
    "    df_pred.reset_index(inplace=True)\n",
    "    df_pred.set_index(df_pred['index'].apply(lambda x: (x - timedelta(days=x.dayofweek))),inplace=True)\n",
    "    df_pred = df_pred['Price_estimated'].to_frame()\n",
    "\n",
    "    # Import prediction data to BI\n",
    "    df_price_model = df_pred.merge(df_prices_non_zero,how='outer',left_on=df_pred.index,right_on=df_prices_non_zero.index).set_index('key_0').reset_index().fillna(0)\n",
    "    df_price_model.rename(columns={'key_0' : 'Date_ref'},errors=\"raise\",inplace=True)\n",
    "    df_price_model.sort_values(by='Date_ref',inplace=True)\n",
    "\n",
    "    return df_price_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions_db(df_price_model,ctry,crop,trade_ctry,regn,ctgr,pkge,crcy,msre,mdel):\n",
    "\n",
    "    ##  Function to upload prices estimated into SQL Server Database   #\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import transformations as transf\n",
    "    import extractions as extract\n",
    "    import config\n",
    "    import pyodbc\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    from datetime import datetime, timedelta\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    from statsmodels.tsa.arima_model import ARIMAResults\n",
    "    from datetime import datetime, timedelta\n",
    "    import stringing as st\n",
    "\n",
    "    connStr = pyodbc.connect(config.db_con)\n",
    "    cursor = connStr.cursor()\n",
    "\n",
    "    #ctry = st.get_comma_values(df_price_model.Country)\n",
    "    #crop = st.get_comma_values(df_price_model.Product)\n",
    "\n",
    "    # Delete all prices predicted which are being predicted (greater than the minimum prediction date) for the country and crop\n",
    "    min_pred_date = min(df_price_model[df_price_model.Price_estimated != 0].Date_ref).date()#.strftime(\"%Y-%m-%d\")\n",
    "    qry_delete = f\"DELETE FROM [Prices].[dbo].[prices_prediction] where [Country] = '{ctry}' and [Product]='{crop}' and [Trade_Country]='{trade_ctry}' and [Category]='{ctgr}' and [Model]='{mdel}' and [Date_price] >= '{min_pred_date}'\"\n",
    "    cursor.execute(qry_delete)\n",
    "\n",
    "    # Load all data with price dates greater than the N last days from today\n",
    "    upd = 0\n",
    "\n",
    "    # https://bytes.com/topic/python/answers/166025-python-mysql-insert-null\n",
    "    for index,row in df_price_model.iterrows():\n",
    "        if row['Price_estimated'] == 0:\n",
    "            price_estimated = None \n",
    "        else: \n",
    "            price_estimated = row['Price_estimated']\n",
    "        if row['Price'] == 0: \n",
    "            price_real = None \n",
    "        else: \n",
    "            price_real = row['Price']\n",
    "        if row['Date_ref'] >= min_pred_date:\n",
    "            cursor.execute(\"INSERT INTO dbo.prices_prediction([Product],[Country],[Trade_Country],[Region],[Category],[Package],[Date_price],[Currency],[Measure],[Model],[Price],[Price_estimated],[Updated]) values (?,?,?,?,?,?,?,?,?,?,?,?,?)\",crop,ctry,trade_ctry,regn,ctgr,pkge,row['Date_ref'],crcy,msre,mdel,row['Price'],row['Price_estimated'],datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "            connStr.commit()\n",
    "            upd += 1\n",
    "\n",
    "    cursor.close()\n",
    "    connStr.close()    \n",
    "\n",
    "    return (print(upd,\" new prices added\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_vols(ctry,crop,trade_ctry,regn,ctgr,pkge,crcy,msre,mdel,start,end):\n",
    "\n",
    "    ##  Function to call a model and make price predictions   ##\n",
    "\n",
    "    import sys\n",
    "    sys.path.insert(0, '../../src')\n",
    "    sys.path.append('../../src/d00_utils')\n",
    "    sys.path.append('../../src/d01_data')\n",
    "    sys.path.append('../../src/d02_processing')\n",
    "    sys.path.append('../../src/d03_modelling')\n",
    "    import transformations as transf\n",
    "    import extractions as extract\n",
    "    import config\n",
    "    import pyodbc\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    import warnings\n",
    "    from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "    from pandas import read_csv\n",
    "    from pandas import datetime\n",
    "    import pmdarima as pm\n",
    "    from pmdarima import model_selection\n",
    "    from datetime import datetime, timedelta\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "    from statsmodels.tsa.arima_model import ARIMAResults\n",
    "    from datetime import datetime, timedelta\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    df_vols = extract.get_volumes(crop,ctry,trade_ctry)\n",
    "    dfNullID = extract.get_null_prices(crop,ctry,trade_ctry,ctgr)\n",
    "    #df_prices_non_zero = df_prices[~df_prices.index.isin(dfNullID.set_index('Date_price').index)]\n",
    "\n",
    "    # load model\n",
    "    ctry_lc = ctry.lower()\n",
    "    crop_lc = crop.lower()\n",
    "    mdel_lc = mdel.lower()\n",
    "    tctr_lc = trade_ctry.lower()\n",
    "    ctgr_lc = ctgr.lower()\n",
    "\n",
    "    model_name = f'../../data/03_models/model_{mdel_lc}_vols_{crop_lc}_{ctry_lc}_{tctr_lc}_{ctgr_lc}.pkl'\n",
    "\n",
    "    try:\n",
    "        ld_model = ARIMAResults.load(model_name)\n",
    "    except FileNotFoundError:\n",
    "        print('No model found')\n",
    "        return\n",
    "        \n",
    "    # make predictions for last year and the following\n",
    "    yr = str(datetime.now().year+1)\n",
    "\n",
    "    #pred = ld_model.get_prediction(start=start, end=end, dynamic=True)\n",
    "    if mdel == 'SARIMA':  \n",
    "        pred = ld_model.get_prediction(start=start, end=end, dynamic=True)\n",
    "        df_pred = pred.predicted_mean.to_frame(name='Volume_estimated')\n",
    "    else: \n",
    "        print(\"Invalid model\") \n",
    "        return\n",
    "    \n",
    "    # Generate only non-zero prices for predicted weeks, based on last-year-no-price weeks \n",
    "    #last_yr_wk_zero = dfNullID[dfNullID['Date_price'].dt.year==int(yr)-2]['Date_price'].dt.week\n",
    "    #df_pred = df_pred[~df_pred.index.week.isin(last_yr_wk_zero)]\n",
    "    \n",
    "    # Mondays and not Sundays as starting day of the week\n",
    "    df_pred.reset_index(inplace=True)\n",
    "    df_pred.set_index(df_pred['index'].apply(lambda x: (x - timedelta(days=x.dayofweek))),inplace=True)\n",
    "    df_pred = df_pred['Volume_estimated'].to_frame()\n",
    "    num = df_pred._get_numeric_data()\n",
    "    num[num < 0] = 0\n",
    "\n",
    "    # Import prediction data to BI\n",
    "    df_volume_model = df_pred.merge(df_vols,how='outer',left_on=df_pred.index,right_on=df_vols.index).set_index('key_0').reset_index().fillna(0)\n",
    "    df_volume_model.rename(columns={'key_0' : 'Date_ref'},errors=\"raise\",inplace=True)\n",
    "    df_volume_model.sort_values(by='Date_ref',inplace=True)\n",
    "\n",
    "    return df_volume_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit5c446d3ae10d4c4b9ddac51de7b948b5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}